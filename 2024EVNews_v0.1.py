# -*- coding: utf-8 -*-
"""2023NewsCrawler_v0.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NaYqL8G-2-1eQPWUgncTNBNyn5UTe9Cp

1.加入google news
2.每個keyword最多出現兩則
"""

"""# **0.General Function**"""
from googletrans import Translator
import requests
from datetime import date, timedelta
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
import sys
import gspread
import requests
import openai
from linebot import LineBotApi
from linebot.models import TextSendMessage
import time
from wordpress_xmlrpc import Client, WordPressPost
from wordpress_xmlrpc.methods.posts import NewPost
from deep_translator import GoogleTranslator
#
import os
from selenium import webdriver
import time
import pandas as pd
import pytz
import feedparser
import html

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
#
chrome_options = webdriver.ChromeOptions()
#chrome_options.binary_location = os.environ.get("GOOGLE_CHROME_BIN")
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-gpu")
driver = webdriver.Chrome(options=chrome_options)



"""
driver.get("https://www.google.com")
print(driver.page_source)
"""
#
def lineNotifyMessage(token, msg):
    headers = {
        "Authorization": "Bearer " + token,
        "Content-Type" : "application/x-www-form-urlencoded"
    }

    payload = {'message': msg }
    r = requests.post("https://notify-api.line.me/api/notify", headers = headers, params = payload)
    return r.status_code



def GTranslateforContent(Content:str ,Country_code):

    '''Country_code translate dest

        'zh-cn': 'chinese (simplified)',
        'zh-tw': 'chinese (traditional)',
        'co': 'corsican',
        'hr': 'croatian',
        'cs': 'czech',
        'da': 'danish',
        'nl': 'dutch',
        'en': 'english',

    for more destintion language type see below link
    https://py-googletrans.readthedocs.io/en/latest/


    '''
    if Country_code =="":
        Country_code = 'zh-tw'
    translator = Translator()
    result = translator.translate(Content, dest=Country_code)
    return result.text

def GTranslateforContent2(Content:str):

    translated = GoogleTranslator(source='auto', target='zh-TW').translate(Content)

    return translated


def remove_html_tags(text):
    # 將 HTML 實體轉換為普通文字
    text = html.unescape(text)
    # 使用正則表達式移除 HTML 標籤
    clean_text = re.sub(r'<.*?>', '', text)
    return clean_text

""""""

collect_en_title = []
collect_link = []
collect_keyword = []

new_list_title = []
new_list_link = []
new_keyword = []

keyword_array = ["Ford","FaradayFuture","Vinfast","Waymo","GM","Cruise","robotaxi","Tesla","Lucid",
           "Toyota","SonyHonda","Honda","Nissan",
           "Volvo","Benz","VAG","Audi","BMW","volkswagen","Stellantis",
           "BYD","Zeeker","Li-Auto","Geely","Nio","Xpeng","GreatWall","Changan","SAICMotor",
           "NVidia","Qualcomm","Intel","NXP","HorizonRobotics","Bosch","Aisin","Hitachi","Denso","Desay","Foxconn","Flextronic","Valeo","Aptiv",
           "Inverter","ADAS","IVI"
           ,"Kia","Hyundai"]

feed_brand_url = ["https://www.google.com/alerts/feeds/12563446222941836209/15952926454595226503",
                  "https://www.google.com/alerts/feeds/12563446222941836209/15952926454595227400",
                  "https://www.google.com/alerts/feeds/12563446222941836209/8245251575210826330",
                  "https://www.google.com/alerts/feeds/12563446222941836209/18337305813738490901",
                  "https://www.google.com/alerts/feeds/12563446222941836209/15301527953058484894",
                  "https://www.google.com/alerts/feeds/12563446222941836209/8245251575210827274",
                  "https://www.google.com/alerts/feeds/12563446222941836209/6270531919427822471",
                  "https://www.google.com/alerts/feeds/12563446222941836209/14219716857154676920",
                  "https://www.google.com/alerts/feeds/12563446222941836209/5367169847606574413",
                  "https://www.google.com/alerts/feeds/12563446222941836209/304141373995844702",
                  "https://www.google.com/alerts/feeds/12563446222941836209/6980634214833720966",
                  "https://www.google.com/alerts/feeds/12563446222941836209/3170915817019476836",
                  "https://www.google.com/alerts/feeds/12563446222941836209/17727018889099757775",
                  "https://www.google.com/alerts/feeds/12563446222941836209/12379976229486810180",
                  "https://www.google.com/alerts/feeds/12563446222941836209/7337995816424024399",
                  "https://www.google.com/alerts/feeds/12563446222941836209/5200254209723481328",
                  "https://www.google.com/alerts/feeds/12563446222941836209/6789679543945960100",
                  "https://www.google.com/alerts/feeds/12563446222941836209/15413560880309697668",
                  "https://www.google.com/alerts/feeds/12563446222941836209/5280258159346850377",
                  "https://www.google.com/alerts/feeds/12563446222941836209/3981482735550362340",
                  "https://www.google.com/alerts/feeds/12563446222941836209/4471530610703659962",
                  "https://www.google.com/alerts/feeds/12563446222941836209/10528019330935726000",
                  "https://www.google.com/alerts/feeds/12563446222941836209/17580191060293741075",
                  "https://www.google.com/alerts/feeds/12563446222941836209/14933425486862855748",
                  "https://www.google.com/alerts/feeds/12563446222941836209/365956415273966064",
                  "https://www.google.com/alerts/feeds/12563446222941836209/4085992709734551175",
                  "https://www.google.com/alerts/feeds/12563446222941836209/9885622520047031582",
                  "https://www.google.com/alerts/feeds/12563446222941836209/16353119383362726805",
                  "https://www.google.com/alerts/feeds/12563446222941836209/12084329268531978276",
                  "https://www.google.com/alerts/feeds/12563446222941836209/12084329268531980491",
                  "https://www.google.com/alerts/feeds/12563446222941836209/1815001472595108513",
                  "https://www.google.com/alerts/feeds/12563446222941836209/15416846028011445335",
                  "https://www.google.com/alerts/feeds/12563446222941836209/9270949065436569760",
                  "https://www.google.com/alerts/feeds/12563446222941836209/9270949065436573584",
                  "https://www.google.com/alerts/feeds/12563446222941836209/5237359591267523771",
                  "https://www.google.com/alerts/feeds/12563446222941836209/5199568533257150373",
                  "https://www.google.com/alerts/feeds/12563446222941836209/3242622044805800994",
                  "https://www.google.com/alerts/feeds/12563446222941836209/1111010771906911047",
                  "https://www.google.com/alerts/feeds/12563446222941836209/5458404612404216743",
                  "https://www.google.com/alerts/feeds/12563446222941836209/15170807423165503637",
                  "https://www.google.com/alerts/feeds/12563446222941836209/2846090626300813125",
                  "https://www.google.com/alerts/feeds/12563446222941836209/2846090626300812807",
                  "https://www.google.com/alerts/feeds/12563446222941836209/8071017713458954097",
                  "https://www.google.com/alerts/feeds/12563446222941836209/5080072315219416992",
                  "https://www.google.com/alerts/feeds/12563446222941836209/10947138298141002145",
                  "https://www.google.com/alerts/feeds/12563446222941836209/11028972963743214259",
                  "https://www.google.com/alerts/feeds/12563446222941836209/15955572733938937353",
                  "https://www.google.com/alerts/feeds/12563446222941836209/3078266409656022434"]

# 將這個 URL 替換為您的 Google Alerts RSS feed URL
#feed_url = "https://www.google.com/alerts/feeds/12563446222941836209/14219716857154676920"

# 解析 RSS Feed
#feed = feedparser.parse(feed_url)

collect_en_title = []
collect_link = []
collect_keyword = []

# 輸出每個新聞項目的標題和連結
for i in range(len(keyword_array)):

  feed = feedparser.parse(feed_brand_url[i])

  for entry in feed.entries:
      collect_en_title.append(remove_html_tags(entry.title))
      collect_link.append(entry.link)
      collect_keyword.append(keyword_array[i])
      print(remove_html_tags(entry.title))
      print(entry.link)
      print(keyword_array[i])
      print('---')


""""""

#selenium區
# 设置Chrome WebDriver路径和URL

#url = "https://carbuzz.com/news"  # 替换为实际的网页链接
#driver.get(url)
#wait = WebDriverWait(driver, 10)
#wait.until(EC.presence_of_element_located((By.CLASS_NAME, "cb-post-preview__title")))

#html = driver.page_source
#soup = BeautifulSoup(html, 'html.parser')
#news_elements = soup.find_all('a', class_='cb-post-preview feed-item')

#for news_element in news_elements:
#    title_element = news_element.find('div', class_='cb-post-preview__title')
#    if title_element:
#        article_title = title_element.text.strip()
#        article_link = "https://carbuzz.com" + news_element['href']  # 替换为实际的网站域名
#        date_element = news_element.find('cb-datetime', class_='cb-post-preview__date')
#        if date_element:
#            date_str = date_element.text.strip()
#            time_match = re.search(r'(\d+)\s+hours\s+ago', date_str)
#            if time_match:
#                hours_ago = int(time_match.group(1))
#                if hours_ago <= 24:
#                    keyword = {"Ford", "FaradayFuture", "Vinfast", "Waymo", "GM", "Cruise", "robotaxi", "Tesla","Lucid",
#                    "Toyota", "Sony-Honda", "Honda", "Nissan", "Volvo", "Benz", "VAG", "Audi", "BMW",
#                    "volkswagen", "Stellantis","BYD", "Zeeker", "Li-Auto","Nio","Xpeng", "Geely", "GreatWall", "Changan",
#                    "SAICMotor", "NVidia", "Qualcomm", "Intel", "NXP", "HorizonRobotics", "Bosch",
#                    "Aisin", "Hitachi", "Denso", "Desay", "Foxconn", "Flextronic", "Valeo", "Aptiv",
#                    "Inverter", "ADAS", "IVI","Kia","Hyundai"}

#                   for x in keyword:
#                     if article_title.find(x) != -1:
#                       print("文章标题:", article_title)
#                       print("文章链接:", article_link)
#                        print("文章關鍵字:",x)
#                        collect_en_title.append(article_title)
#                        collect_link.append(article_link)
#                        collect_keyword.append(x)

#                else:
#                    print("文章不是24小时内的新闻")

#print('爬完carbuzz新聞池')

#try:
#    driver.get("https://www.google.com")
#    print("Page title was '{}'".format(driver.title))
#finally:
#    driver.quit()

# 打开网站
driver.get("https://www.drive.com.au/news/")  # 将URL替换为目标网站的URL

# 等待页面加载完成，可以根据实际情况调整等待时间
wait = WebDriverWait(driver, 10)
wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.article-thumbnail')))

# 获取页面上的所有新闻元素
news_elements = driver.find_elements(By.CSS_SELECTOR, '.article-thumbnail')

# 获取当前日期和时间
current_datetime = datetime.now()

# 遍历所有新闻元素
for news_element in news_elements:
    # 获取新闻标题
    title = news_element.find_element(By.CSS_SELECTOR, '.title').text

    # 获取新闻链接
    link = news_element.find_element(By.CSS_SELECTOR, 'a[data-cy="review-permalink"]').get_attribute('href')

    # 获取新闻发布时间
    date_text = news_element.find_element(By.CSS_SELECTOR, '.date').text

    # 解析新闻发布时间
    news_date = datetime.strptime(date_text, '%d %b %Y')

    # 计算新闻发布时间与当前时间的差值
    time_difference = current_datetime - news_date

    # 判断新闻是否在24小时内发布
    if time_difference <= timedelta(hours=24):

        keyword = {"Ford", "FaradayFuture", "Vinfast", "Waymo", "GM", "Cruise", "robotaxi", "Tesla","Lucid",
                    "Toyota", "Sony-Honda", "Honda", "Nissan", "Volvo", "Benz", "VAG", "Audi", "BMW",
                    "volkswagen", "Stellantis","BYD", "Zeeker", "Li-Auto","Nio","Xpeng", "Geely", "GreatWall", "Changan",
                    "SAICMotor", "NVidia", "Qualcomm", "Intel", "NXP", "HorizonRobotics", "Bosch",
                    "Aisin", "Hitachi", "Denso", "Desay", "Foxconn", "Flextronic", "Valeo", "Aptiv",
                    "Inverter", "ADAS", "IVI","Kia","Hyundai"}

        for x in keyword:
            if title.find(x) != -1:
                print("文章标题:", title)
                print("文章链接:", link)
                print("文章關鍵字:",x)
                collect_en_title.append(title)
                collect_link.append(link)
                collect_keyword.append(x)


# 关闭浏览器
#driver.quit()
print('爬完drive.com新聞池')

# 打开网站首页
driver.get("https://cleantechnica.com/tag/tesla/")

# 等待标题加载完成
title_element = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, "cm-entry-title"))
)

# 获取标题和链接
title = title_element.text
link = title_element.find_element(By.TAG_NAME, "a").get_attribute("href")

# 获取发布时间
time_element = driver.find_element(By.CLASS_NAME, "cm-post-date")
time = time_element.find_element(By.TAG_NAME, "time").get_attribute("datetime")

# 将文章发布时间转换为带有时区信息的datetime对象
article_time = datetime.strptime(time, "%Y-%m-%dT%H:%M:%S%z")

# 获取当前时间，并指定时区信息为UTC
current_time = datetime.now(pytz.utc)

# 判断是否为24小时内的新闻
is_within_24_hours = current_time - article_time < timedelta(days=1)

if is_within_24_hours:
    keyword = {"Ford", "FaradayFuture", "Vinfast", "Waymo", "GM", "Cruise", "robotaxi", "Tesla","Lucid",
                    "Toyota", "Sony-Honda", "Honda", "Nissan", "Volvo", "Benz", "VAG", "Audi", "BMW",
                    "volkswagen", "Stellantis","BYD", "Zeeker", "Li-Auto","Nio","Xpeng", "Geely", "GreatWall", "Changan",
                    "SAICMotor", "NVidia", "Qualcomm", "Intel", "NXP", "HorizonRobotics", "Bosch",
                    "Aisin", "Hitachi", "Denso", "Desay", "Foxconn", "Flextronic", "Valeo", "Aptiv",
                    "Inverter", "ADAS", "IVI","Kia","Hyundai"}

    for x in keyword:
        if title.find(x) != -1:
            print("文章标题:", title)
            print("文章链接:", link)
            print("文章關鍵字:",x)
            collect_en_title.append(title)
            collect_link.append(link)
            collect_keyword.append(x)
else:
    print("这不是24小时内的新闻。")

# 关闭浏览器
#driver.quit()
print('爬完cleantech新聞池')



# 打开网页
url = "https://www.automotiveworld.com/news-releases/"
driver.get(url)

# 显示等待，等待新闻加载完成
wait = WebDriverWait(driver, 10)
element = wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'article-tile')))

# 获取当前日期和昨天的日期
current_time = datetime.now()
yesterday_time = current_time - timedelta(days=1)

# 获取所有新闻元素
news_tiles = driver.find_elements(By.CLASS_NAME, 'article-tile')

# 遍历每个新闻元素
for news_tile in news_tiles:
    # 获取新闻日期元素和链接元素
    date_element = news_tile.find_element(By.CLASS_NAME, 'date')
    link_element = news_tile.find_element(By.TAG_NAME, 'a')

    # 解析日期字符串
    date_str = date_element.text
    news_date = datetime.strptime(date_str, '%B %d, %Y')

    # 判断新闻是否为当天或昨天的新闻
    if news_date.date() == current_time.date() or news_date.date() == yesterday_time.date():
        # 获取新闻标题和链接
        title = link_element.get_attribute('title')
        link = link_element.get_attribute('href')

        keyword = {"Ford", "FaradayFuture", "Vinfast", "Waymo", "GM", "Cruise", "robotaxi", "Tesla","Lucid",
                    "Toyota", "Sony-Honda", "Honda", "Nissan", "Volvo", "Benz", "VAG", "Audi", "BMW",
                    "volkswagen", "Stellantis","BYD", "Zeeker", "Li-Auto","Nio","Xpeng", "Geely", "GreatWall", "Changan",
                    "SAICMotor", "NVidia", "Qualcomm", "Intel", "NXP", "HorizonRobotics", "Bosch",
                    "Aisin", "Hitachi", "Denso", "Desay", "Foxconn", "Flextronic", "Valeo", "Aptiv",
                    "Inverter", "ADAS", "IVI","Kia","Hyundai"}

        for x in keyword:
            if title.find(x) != -1:
                print("文章标题:", title)
                print("文章链接:", link)
                print("文章關鍵字:",x)
                collect_en_title.append(title)
                collect_link.append(link)
                collect_keyword.append(x)

# 关闭浏览器
#driver.quit()
print('爬完automotiveworld新聞池')



# 打開網頁
url = "https://www.autoexpress.co.uk/car-news"
driver.get(url)

# 等待頁面加載完成
wait = WebDriverWait(driver, 20)
wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'polaris__article-card')))

# 獲取當前時間
current_time = datetime.now()

# 獲取所有新聞元素
news_items = driver.find_elements(By.CLASS_NAME, 'polaris__article-card')

# 遍歷每個新聞元素
for news_item in news_items:
    # 獲取日期元素
    date_element = news_item.find_element(By.CLASS_NAME, 'polaris__date')

    # 解析日期字符串
    date_str = date_element.get_attribute('innerHTML')

    # 判斷是否為24小時內的新聞
    if date_str:
        # 解析日期
        news_date = datetime.strptime(date_str.strip(), '%d %b %Y')

        # 判斷是否為24小時內的新聞
        if current_time - news_date < timedelta(hours=24):
            # 獲取新聞標題元素
            title_element = news_item.find_element(By.CSS_SELECTOR, '.polaris__article-card--title span')

            # 獲取新聞標題
            news_title = title_element.get_attribute('innerHTML')

            # 獲取新聞連結
            link_element = news_item.find_element(By.CLASS_NAME, 'polaris__article-card--link')
            news_link = link_element.get_attribute('href')

            keyword = {"Ford", "FaradayFuture", "Vinfast", "Waymo", "GM", "Cruise", "robotaxi", "Tesla","Lucid",
                    "Toyota", "Sony-Honda", "Honda", "Nissan", "Volvo", "Benz", "VAG", "Audi", "BMW",
                    "volkswagen", "Stellantis","BYD", "Zeeker", "Li-Auto","Nio","Xpeng", "Geely", "GreatWall", "Changan",
                    "SAICMotor", "NVidia", "Qualcomm", "Intel", "NXP", "HorizonRobotics", "Bosch",
                    "Aisin", "Hitachi", "Denso", "Desay", "Foxconn", "Flextronic", "Valeo", "Aptiv",
                    "Inverter", "ADAS", "IVI","Kia","Hyundai"}

            for x in keyword:
                if news_title.find(x) != -1:
                    print("文章标题:", news_title)
                    print("文章链接:", news_link)
                    print("文章關鍵字:",x)
                    collect_en_title.append(news_title)
                    collect_link.append(news_link)
                    collect_keyword.append(x)

# 關閉瀏覽器
#driver.quit()
print('爬完autoexpress新聞池')

url = "https://www.wardsauto.com/industry-news"
driver.get(url)

# 等待頁面加載完成
wait = WebDriverWait(driver, 10)
wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'article-teaser__header')))


# 獲取當前時間
current_time = datetime.now()

# 使用 class name 選擇器獲取所有文章元素
news_items = driver.find_elements(By.CLASS_NAME, 'article-teaser__header')


# 遍历每个文章元素
for news_item in news_items:
    try:
        # 获取文章标题和链接
        title_element = news_item.find_element(By.TAG_NAME, 'a')
        news_title = title_element.get_attribute('innerHTML').strip()
        news_link = title_element.get_attribute('href')
        news_title = news_title.replace('<i class="fa "></i>', '')


        # 获取文章日期
        date_element = news_item.find_element(By.CLASS_NAME, 'date-display-single')
        date_str = date_element.get_attribute('innerHTML').strip()
        news_date = datetime.strptime(date_str, '%b %d, %Y')

        # 检查是否是24小时内或昨天的新闻
        time_diff = current_time - news_date
        if time_diff <= timedelta(hours=24) or (time_diff <= timedelta(hours=48) and time_diff > timedelta(hours=24)):

            keyword = {"Ford", "FaradayFuture", "Vinfast", "Waymo", "GM", "Cruise", "robotaxi", "Tesla","Lucid",
                    "Toyota", "Sony-Honda", "Honda", "Nissan", "Volvo", "Benz", "VAG", "Audi", "BMW",
                    "volkswagen", "Stellantis","BYD", "Zeeker", "Li-Auto","Nio","Xpeng", "Geely", "GreatWall", "Changan",
                    "SAICMotor", "NVidia", "Qualcomm", "Intel", "NXP", "HorizonRobotics", "Bosch",
                    "Aisin", "Hitachi", "Denso", "Desay", "Foxconn", "Flextronic", "Valeo", "Aptiv",
                    "Inverter", "ADAS", "IVI","Kia","Hyundai"}

            for x in keyword:
                if news_title.find(x) != -1:
                    print("文章标题:", news_title)
                    print("文章链接:", news_link)
                    print("文章關鍵字:",x)
                    collect_en_title.append(news_title)
                    collect_link.append(news_link)
                    collect_keyword.append(x)

    except Exception as e:
        print("处理新闻时出错:", e)

# 关闭浏览器
driver.quit()
print('爬完wardsauto新聞池')
print(len(collect_en_title),len(collect_link),len(collect_keyword))
#time.sleep(5)
#
""""""

import gspread
from oauth2client.service_account import ServiceAccountCredentials as SAC
import time

#Structure
#1.google news之外的池子用ex.{ford ev}，加ev關鍵字來爬，google可以在關鍵字中用"-"來排除篩選
#2.我們先分區域(category)廠牌(tag)，來分類，但每一篇post都是以當日抓取的單一關鍵字產生一篇，所以一天可能會有30-40篇
#3.在mailpoet放入5個（us.eu.jp.tesla.platform-tier1.shipment-ev market share)post元件，他會自動更新(notification類別文章)
#4.還是可以用openai篩選一下當天文章，怕量會太多，評分看要不要留著
#5.還是必須將資料寫入google sheet一方面給老路用，一方面怕wordpress網站掛掉

#keyword list
#US: Ford(lear,flex,jabil),FaradayFuture,Vinfast,Waymo,GM,Cruise,robotaxi,Tesla
#JP: Toyota,Sony – Honda,Nissan
#EU: Volvo,Benz,VAG,VW,Audi,BMW,Stellantis
#CN: Zeeker,lixiang,geely,Great Wall(長城汽車),Changan,SAIC Motor(上汽)
#Platefrom: NVidia,NXP,Horizon Robotics,Qualcomm,Intel
#Tier1: Bosch,Aisin,Hitachi,denso,desay,Foxconn,Flextronic,Valeo,Aptiv
#Inverter,ADAS,IVI

#先用關鍵字找許多的文章，在透過分類關鍵字找標題或內文是否有符合就給他貼上標籤，最後在貼到wordpress
#實驗：較精確地透過關鍵字找到文章，一天要幾篇？太多篇(300篇)網站會塞爆

keyword = {"Ford", "FaradayFuture", "Vinfast", "Waymo", "GM", "Cruise", "robotaxi", "Tesla","Lucid",
            "Toyota", "Sony-Honda", "Honda", "Nissan", "Volvo", "Benz", "VAG", "Audi", "BMW",
            "volkswagen", "Stellantis","BYD", "Zeeker", "Li-Auto","Nio","Xpeng", "Geely", "GreatWall", "Changan",
            "SAICMotor", "NVidia", "Qualcomm", "Intel", "NXP", "HorizonRobotics", "Bosch",
            "Aisin", "Hitachi", "Denso", "Desay", "Foxconn", "Flextronic", "Valeo", "Aptiv",
            "Inverter", "ADAS", "IVI","Kia","Hyundai"}



"""# **2.EV News Pool Crawler - Digitimes** : https://www.digitimes.com/ev/ , 有篩選時間與關鍵字

## 篩選版
"""
for key in keyword:
  url = f"https://www.digitimes.com/search/results.asp?q={key}&ch=13"
  domain = "https://www.digitimes.com"

  response = requests.get(url)
  html_content = response.text

  soup = BeautifulSoup(html_content, 'html.parser')
  search = soup.find("div", {"class": "search-pane"})

  title_final = []
  link_final = []
  date_final = []

  articles = search.find_all("div", {"class": "row"})
  final_soup = []

  for article in articles:
    if article.find("div",{"class","col-12 pm-0"}) != None:
      final_soup.append(article.find("div",{"class","col-12 pm-0"}))


  for test in final_soup:
    if test.find('div', class_='date') != None:
      date = test.find('div', class_='date').text.strip()
      date_final.append(date)

  for test2 in final_soup:
    title_link = test2.find('a')
    if title_link != None:
      title = title_link.text
      link = title_link['href']
      link2 = domain + link

      if "https://www.digitimes.com/tag/" not in link2:
        title_final.append(title)
        link_final.append(link2)
        #print(key,title,link2)


  for i in range(len(title_final)):
    now = datetime.now()
    yesterday = datetime.now() - timedelta(days=1)

    if date_final[i] is not None:
      date_obj = datetime.strptime(date_final[i], '%A %d %B %Y')
      new_date_str = date_obj.strftime('%Y-%m-%d')

      if str(new_date_str) == str(now.date()):
        print(key,title_final[i],link_final[i],date_final[i])
        collect_en_title.append(title_final[i])
        collect_link.append(link_final[i])
        collect_keyword.append(key)

"""# **3.EV News Pool Crawler - JustCar** : https://www.just-auto.com/news/ , 有篩選時間與關鍵字

## 篩選版
"""
for key in keyword:
  url = f"https://www.just-auto.com/s?wpsolr_q={key}&wpsolr_sort=sort_by_date_desc"
  #url = f"https://newsapi.org/v2/everything?q={query}&from={yesterday}&to={today}&apiKey=d5820cbb68a24bc7b8ae4e84aaf26a21"
  response = requests.get(url)
  html_content = response.text

  soup = BeautifulSoup(html_content, 'html.parser')
  articles = soup.find_all("article", {"class": "cell feature grid-x border-bottom"})
  for article in articles:
    date_element = article.find("span", {"class": "pcat date mb-small"})
    if date_element is not None:
      date = date_element.text.strip().replace("|", "").strip()
      date_object = datetime.strptime(date, "%d %b %Y")

      now = datetime.now()
      yesterday = datetime.now() - timedelta(days=1)

      if date_object.date() == now.date():

        title_element = article.find("h3").find("a")
        title = title_element.text.strip()
        title_link = title_element["href"]

        print("Keyword: ",key)
        print("Date: ", date)
        print("Title: ", title)
        print("Title link: ", title_link)

      elif date_object.date() == yesterday.date():

        title_element = article.find("h3").find("a")
        title = title_element.text.strip()
        title_link = title_element["href"]

        # Print the extracted data
        print("Keyword: ",key)
        print("Date: ", date)
        print("Title: ", title)
        print("Title link: ", title_link)

        collect_en_title.append(title)
        collect_link.append(title_link)
        collect_keyword.append(key)

"""# **4.EV News Pool Crawler - AutoWeek** : https://www.autoweek.com/news/ , 有篩選時間與關鍵字

### 篩選版
"""
url_ori = "https://www.autoweek.com"

for key in keyword:
  url = f"https://www.autoweek.com/search/?q={key}"

  response = requests.get(url)
  html_content = response.text

  soup = BeautifulSoup(html_content, 'html.parser')

  articles = soup.find_all("a", {"class": " enk2x9t2 css-zgpw49 epl65fo4"})

  for article in articles:
    date_element = article.find("div", {"class": "css-dthshy e1rluvgc1"})

    if date_element is not None:
      date_object = datetime.strptime(date_element.text, "%b %d, %Y")
      now = datetime.now()
      yesterday = datetime.now() - timedelta(days=1)

      if date_object.date() == yesterday.date():
        link = article["href"]
        final_link = url_ori+link

        title = article.find("h2")("span")[1]
        title_ana = re.sub(r'<.*?>', '', str(title))

        print(key,date_object,final_link,title_ana)

        collect_en_title.append(title_ana)
        collect_link.append(final_link)
        collect_keyword.append(key)

"""# **10.EV News Pool Crawler - Electrek** : https://electrek.co/?s=Latest&sort_order=desc , 有篩選時間與關鍵字

### 篩選版
"""

for key in keyword:
  url = f"https://electrek.co/?s={key}&sort_order=desc"
  response = requests.get(url)
  html_content = response.text

  soup = BeautifulSoup(html_content, 'html.parser')

  results = soup.find_all("div", {"class": "article__content"})

  for result in results:
    date = result.find_all("span", {"class": "meta__post-date"})
    date_string = str(date).replace('[', '').replace(']', '').replace('<span class="meta__post-date">', '').replace('</span>', '')

    now = datetime.now()
    yesterday = datetime.now() - timedelta(days=1)

    if date_string is not None:
      if "ago" in date_string:
        date_object = now.date()
      else:
        date_object = datetime.strptime(date_string, "%b %d %Y").date()

      if date_object == now.date():
        title = result.find('a',{'class':'article__title-link'}).text
        link = result.find('a',{'class':'article__title-link'})['href']

        if "electrek" in link:
          print(key,title,link,date_object,"match")

          collect_en_title.append(title)
          collect_link.append(link)
          collect_keyword.append(key)

"""# **11.EV News Pool Crawler - Insideevs** : https://insideevs.com/news/category/news/ , 有篩選時間與關鍵字

### 篩選版
"""

for key in keyword:
  url = f"https://insideevs.com/search/?q={key}&f=news"
  response = requests.get(url)
  html_content = response.text

  soup = BeautifulSoup(html_content, 'html.parser')

  results = soup.find_all("div", {"class": "item wcom "})

  for result in results:
    date = result.find_all("span", {"class": "date"})

    match = re.search('>(.*?)<', str(date))
    if match:
        time_string = match.group(1)

    if len(time_string) == 0:
      real_date = now.date()
    else:
      real_date = datetime.strptime(time_string, '%d %B %Y').date()

    if real_date == now.date():

      title = result.find("h3").text
      new_url = "https://insideevs.com" + result.find("a")["href"]

      print(key,real_date,title,new_url)

      collect_en_title.append(title)
      collect_link.append(new_url)
      collect_keyword.append(key)


import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re

from googlesearch import search
from googleapiclient.discovery import build
#
def fetch_top_search_results(query, num_results=10):
    search_results = search(query, num_results=num_results)
    return list(search_results)  # 将生成器转换为列表

def fetch_first_search_result(query):
    top_results = fetch_top_search_results(query, num_results=10)
    first_result = top_results[0] if top_results else None  # 现在可以安全地访问第一项
    return first_result

def google_search(search_term, api_key, cse_id, **kwargs):
    service = build("customsearch", "v1", developerKey=api_key)
    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()
    return res['items']

#Google Search engin key
api_key = 'AIzaSyCSD1qGye1XPHduHQPLYaYUP2r9w3eNrBw'
cse_id = 'd711d5eebac8d4001'

# 要抓取的網址
#關鍵字：EV news
url = "https://news.google.com/search?q=EV%20news%20when%3A1d&hl=en-US&gl=US&ceid=US%3Aen"
#關鍵字：Top EV news
#https://news.google.com/search?q=Top%20EV%20news&hl=en-US&gl=US&ceid=US%3Aen
#之後有需要可以透過每個關鍵字產生網誌，還要注意google news html格式會變，屆時程式碼會出錯l

# 進行網頁請求
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# 定義關鍵字
keywords_for_this = {"Ford", "FaradayFuture", "Vinfast", "Waymo", "GM", "Cruise", "robotaxi", "Tesla", "Lucid",
            "Toyota", "Sony-Honda", "Honda", "Nissan", "Volvo", "Benz", "VAG", "Audi", "BMW",
            "volkswagen", "Stellantis","BYD", "Zeeker", "Li-Auto","Nio","Xpeng", "Geely", "GreatWall", "Changan",
            "SAICMotor", "NVidia", "Qualcomm", "Intel", "NXP", "HorizonRobotics", "Bosch",
            "Aisin", "Hitachi", "Denso", "Desay", "Foxconn", "Flextronic", "Valeo", "Aptiv",
            "Inverter", "ADAS", "IVI","Kia","Hyundai"}

# 解析網頁以獲取新聞標題和時間戳記
articles = soup.find_all('article', class_='IFHyqb DeXSAc')
for article in articles:
    title_tag = article.find('a', class_='JtKRv')
    time_tag = article.find('time', class_='hvbAAd')

    if title_tag and time_tag:
        title = title_tag.text.strip()
        datetime_str = time_tag['datetime']

        # 轉換 datetime 字串為 datetime 對象
        published_time = datetime.fromisoformat(datetime_str[:-1])

        # 判斷是否在過去 24 小時內發布且包含至少一個關鍵字
        if datetime.now() - published_time <= timedelta(days=1):
            found_keyword = None
            for keyword in keywords_for_this:
                if keyword.lower() in title.lower():
                    found_keyword = keyword
                    break

            if found_keyword:
                try:
                    results = google_search(title, api_key, cse_id, num=1)
                    collect_en_title.append(title)
                    collect_keyword.append(found_keyword)
                    collect_link.append(results[0]['link'])

                except:
                    print("超出每日查询限额，无法处理标题")
                    break  # 或者使用 continue 来跳过当前迭代，而不是完全停止循环



# 輸出結果
for i in range(len(collect_en_title)):
    print(f"Title: {collect_en_title[i]}")
    print(f"Keywords: {collect_keyword[i]}")
    print(f"Link:{collect_link[i]}")
    print()

"""# **13 google news"""
from GoogleNews import GoogleNews

for key in keyword:
  now = datetime.now()
  yesterday = datetime.now() - timedelta(days=1)

  date_string_now = now.strftime("%m/%d/%Y")
  date_string_yesterday = yesterday.strftime("%m/%d/%Y")


  googlenews = GoogleNews()

  #選擇搜尋語言
  googlenews.setlang('en')
  #清空搜尋結果陣列
  googlenews.clear()
  #設定搜尋日期 期間
  googlenews.setTimeRange(date_string_yesterday,date_string_now)
  #設定語言編碼
  googlenews.setencode('utf-8')
  #關鍵字搜尋
  googlenews.search(key)
  #搜尋頁數
  times = 1
  #開始進行搜尋 雙層for loop 先跑頁面數量 在跑抓取功能
  for i in range(times):
      #再次清空陣列
      googlenews.clear()
      #進入 i 頁面抓取所有新聞
      googlenews.getpage(i)
      #得到抓取結果存到 result list中
      result = googlenews.result()
      #print(len(result))
      for n in range(len(result)):
          temp = result[n]
          #從result list得到該新聞的連結
          #假如你想要得到title 就使用temp['title']
          print(temp['title'])
          #print(temp['link'])
          #把url存到list中 使用append塞入
          #UrlList.append(temp['link'])
          #temp['link'] = re.sub("/url?esrc=s&q=&rct=j&sa=U&url=","",temp['link'])
          Mystring = temp['link'].replace("/url?esrc=s&q=&rct=j&sa=U&url=","")
          print(Mystring)

          collect_en_title.append(temp['title'])
          collect_link.append(Mystring)
          collect_keyword.append(key)


"""# **整合區**"""
new_list_title = []
new_list_link = []
new_keyword = []
#
r_title = []
r_link = []
r_key = []

for x in range(len(collect_en_title)):
  if "..." not in collect_en_title[x]:
    if len(collect_en_title[x]) > 20:
      r_title.append(collect_en_title[x])
      r_link.append(collect_link[x])
      r_key.append(collect_keyword[x])

for i in range(len(r_title)):
    redundant = False
    for j in range(i+1, len(r_title)):
        if r_title[i] == r_title[j]:
            redundant = True
            break
    if not redundant:
        new_list_title.append(r_title[i])
        new_list_link.append(r_link[i])
        all_new_keyword = r_key[i]
        new_keyword.append(all_new_keyword)

for x in range(len(collect_en_title)):
  print(collect_en_title[x])
  print(collect_link[x])
  print(collect_keyword[x])

print(len(new_list_title),len(new_list_link),len(new_keyword))


anti_keywords = {"nascar", "daytona", "report", "analysis", "wind", "finance", "yahoo", "marketwatch", "Starbucks", "stocks", "stock", "mortgage", "bath", "facebook", "instagram",
                 "fda", "gold", "inflation", "naturalgas", "appstore", "mining", "vast", "bondguarantors", "Covid", "playoff", "sport", "Hertz", "rental","Polestar"}

count = 0
del_list = []
for x in range(len(new_list_title)):
    z2 = str(new_list_title[x]).lower()
    for keyword in anti_keywords:
        z1 = keyword.lower()
        if z1 in z2:
            count = count + 1
            print(z1, z2, x, count)
            del_list.append(x)

del_list = sorted(del_list, reverse=True)  # 以相反順序排序，以避免索引錯誤
for index in del_list:
    del new_list_title[index]
    del new_list_link[index]
    del new_keyword[index]


import openai
import time
import threading

pass_list_title = []
pass_list_link = []
pass_keyword = []

unpass_list_title = []
unpass_list_link = []
unpass_keyword = []

openai.api_key = "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"

new_title_relate = []

#relate_text = ";Is it automotive news (just only replay me true or false)?"
relate_text = ";Is it automotive news (only replay me true or false)?If it's not news related to automotive electronics or automotive matters, especially sports-related, just print false."

# 定义一个函数来处理每个新闻链接
def process_news(index, title, link, keyword):
    try:
        # 使用 OpenAI API 进行聊天推断
        response = openai.ChatCompletion.create(
            model="ft:gpt-3.5-turbo-0613:personal::8wm6J9mX",
            max_tokens=1024,
            temperature=0.1,
            messages=[
                {"role": "user", "content": title + relate_text},
            ]
        )

        # 如果返回结果为 true，则将该新闻链接添加到 pass 列表中
        if response.choices[0].message.content.lower() == "true":
            print("Title:", title)
            print("Link:", link)
            print("Keyword:", keyword)

            pass_list_title.append(title)
            pass_list_link.append(link)
            pass_keyword.append(keyword)
        else:
            unpass_list_title.append(title)
            unpass_list_link.append(link)
            unpass_keyword.append(keyword)
    except Exception as e:
        print("Error occurred for", title, ":", e)

# 遍历每个新闻链接，并启动一个线程来处理每个链接
threads = []
for i in range(len(new_list_title)):
    thread = threading.Thread(target=process_news, args=(i, new_list_title[i], new_list_link[i], new_keyword[i]))
    thread.start()
    threads.append(thread)

# 等待所有线程执行完成
for thread in threads:
    thread.join()


import openai
import time
import threading

unpass_list_title2 = []
unpass_list_link2 = []
unpass_keyword2 = []

openai.api_key = "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"

relate_text = ";Is it automotive news (only replay me true or false)?If it's not news related to automotive electronics or automotive matters, especially sports-related, just print false."

# 定义一个函数来处理每个新闻链接
def process_news(index, title, link, keyword):
    try:
        # 使用 OpenAI API 进行聊天推断
        response = openai.ChatCompletion.create(
            model="ft:gpt-3.5-turbo-0125:personal::92YBEwT5",
            max_tokens=1024,
            temperature=0.1,
            messages=[
                {"role": "user", "content": title + relate_text},
            ]
        )

        # 如果返回结果为 true，则将该新闻链接添加到 pass 列表中
        if response.choices[0].message.content.lower() == "true":
            print("Title:", title)
            print("Link:", link)
            print("Keyword:", keyword)

            pass_list_title.append(title)
            pass_list_link.append(link)
            pass_keyword.append(keyword)
        else:
            unpass_list_title2.append(title)
            unpass_list_link2.append(link)
            unpass_keyword2.append(keyword)
    except Exception as e:
        print("Error occurred for", title, ":", e)

# 遍历每个新闻链接，并启动一个线程来处理每个链接
threads = []
for i in range(len(unpass_list_title)):
    thread = threading.Thread(target=process_news, args=(i, unpass_list_title[i], unpass_list_link[i], unpass_keyword[i]))
    thread.start()
    threads.append(thread)

# 等待所有线程执行完成
for thread in threads:
    thread.join()

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# 定义要筛选的单个单词和短语
unrelated_keywords = ["nascar","daytona","report","analysis","wind","finance","yahoo","marketwatch","Starbucks","stocks","stock","mortgage","bath","facebook","instagram"
,"fda","gold","inflation","naturalgas","appstore","mining","vast","bondguarantors","Covid","playoff","sport","ship","sony","Hertz","rebounds","nba","Ticker","scooter","solar"
,"TwitterSource","rental","Polestar"]  # 单个单词

phrases = ["cruise year","Super Bowl","Tesla Optimus robot","desktop computers","Shipping Port","vaginal tightening gel"]  # 短语

# 确保已经下载了NLTK的stopwords和punkt资源
nltk.download('punkt')
nltk.download('stopwords')

# 进行新闻筛选
final_filtered_titles = []
final_filtered_links = []
final_filtered_keywords = []

# 将文本分词并过滤停用词
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [w.lower() for w in word_tokens if not w.lower() in stop_words]
    return filtered_text

# 判断标题是否包含非相关关键词
def is_unrelated(title, unrelated_keywords):
    title_words = preprocess_text(title)
    return any(keyword in title_words for keyword in unrelated_keywords)

# 判断标题是否包含特定的短语
def does_not_contain_phrase(title, phrases):
    title_lower = title.lower()
    return all(phrase not in title_lower for phrase in phrases)

for title, link, keyword in zip(pass_list_title, pass_list_link, pass_keyword):
    if not is_unrelated(title, unrelated_keywords) and does_not_contain_phrase(title, phrases):
        final_filtered_titles.append(title)
        final_filtered_links.append(link)
        final_filtered_keywords.append(keyword)

# 输出结果
print("符合条件的新闻数量：", len(final_filtered_titles))
for title, link, keyword in zip(final_filtered_titles, final_filtered_links, final_filtered_keywords):
    print(f"标题: {title}, 链接: {link}, 关键字: {keyword}")



pass_list_title_zh = []

for i in range(len(final_filtered_titles)):
    pass_list_title_zh.append(GTranslateforContent2(final_filtered_titles[i]))

for j in range(len(final_filtered_titles)):
  print(final_filtered_titles[j])
  print(final_filtered_links[j])
  print(pass_list_title_zh[j])
  print(final_filtered_keywords[j])
  print('--------------------------')


  # 判断标题中是否包含关键字，并修正关键字
def contains_and_fix_keyword(title, keywords):
    # 分词
    words = title.split()
    # 检查是否有关键字
    for keyword in keywords:
        if re.search(r'\b{}\b'.format(re.escape(keyword)), title, re.IGNORECASE):
            return keyword
    return None

# 判斷標題內是否有該關鍵字
tag_key = {"Ford","FaradayFuture","Vinfast","Waymo","GM","Cruise","robotaxi","Tesla","Lucid",
           "Toyota","Sony-Honda","Honda","Nissan","Volvo","Benz","VAG","Audi","BMW","volkswagen","Stellantis",
           "BYD","Zeeker","Li-Auto","Geely","Nio","Xpeng","GreatWall","Changan","SAICMotor",
           "NVidia","Qualcomm","Intel","HorizonRobotics","NXP","Bosch","Aisin","Hitachi","Denso","Desay","Foxconn","Flextronic","Valeo","Aptiv",
           "Inverter","ADAS","IVI","Kia","Hyundai"}

print(len(final_filtered_titles))

for i in range(len(final_filtered_titles)):
    title = final_filtered_titles[i]
    keyword = final_filtered_keywords[i]

    matched_keyword = contains_and_fix_keyword(title.lower(), tag_key)
    if matched_keyword:
        final_filtered_keywords[i] = matched_keyword
    else:
        print("未匹配到任何关键字")

    print("新聞標題:", title)
    print("修正key:", final_filtered_keywords[i])
    print()

#這邊把所有抓到的文章多兩個標籤來做文章分群分類

#keyword = {"Ford EV","FaradayFuture EV","Vinfast EV","Waymo","GM EV","Cruise EV","robotaxi","Tesla"
#,"Toyota EV","Sony–Honda EV","Honda EV","Nissan EV","Volvo EV","Benz EV","VAG EV","Audi EV","BMW EV","volkswagen EV","Stellantis EV"
#,"NVidia EV","Qualcomm EV","Intel EV","NXP EV","Bosch EV","Aisin EV","Hitachi EV","Denso EV","Desay EV","Foxconn EV","Flextronic EV","Valeo EV","Aptiv EV"
#,"Inverter EV","ADAS","IVI EV"}

#keyword list (42)
#US: Ford(lear,flex,jabil),FaradayFuture,Vinfast,Waymo,GM,Cruise,robotaxi,Tesla
#JP: Toyota,Sony – Honda,Nissan
#EU: Volvo,Benz,VAG,VW,Audi,BMW,Stellantis
#CN: Zeeker,lixiang,geely,Great Wall(長城汽車),Changan,SAIC Motor(上汽)
#Platefrom: NVidia,NXP,Horizon Robotics,Qualcomm,Intel
#Tier1: Bosch,Aisin,Hitachi,denso,desay,Foxconn,Flextronic,Valeo,Aptiv
#Inverter,ADAS,IVI

#catelog只有七類(US,EU,JP,CH,Platform,Tier1,Component)
article_catelog = []

catelog_us = {"Ford","FaradayFuture","Vinfast","Waymo","GM","Cruise","Tesla","robotaxi","Lucid"}
catelog_jp = {"Toyota","Sony-Honda","Honda","Nissan"}
catelog_eu = {"Volvo","Benz","volkswagen","Audi","BMW","Stellantis","VAG"}
catelog_cn = {"BYD","Zeeker","Li-Auto","Geely","Nio","Xpeng","GreatWall","Changan","SAICMotor"}
catelog_platform = {"NVidia","NXP","Qualcomm","Intel","HorizonRobotics"}
catelog_tier1 = {"Bosch","Aisin","Hitachi","Denso","Desay","Foxconn","Flextronic","Valeo","Aptiv"}
catelog_component = {"Inverter","ADAS","IVI"}
catelog_Korea = {"Kia","Hyundai"}

#標籤可以多個，解析文章標題來貼標籤
article_tag = []

tag_key = {"Ford","FaradayFuture","Vinfast","Waymo","GM","Cruise","robotaxi","Tesla","Lucid"
,"Toyota","Sony-Honda","Honda","Nissan","Volvo","Benz","VAG","Audi","BMW","volkswagen","Stellantis"
,"BYD","Zeeker","Li-Auto","Geely","Nio","Xpeng","GreatWall","Changan","SAICMotor"
,"NVidia","Qualcomm","Intel","HorizonRobotics","NXP","Bosch","Aisin","Hitachi","Denso","Desay","Foxconn","Flextronic","Valeo","Aptiv"
,"Inverter","ADAS","IVI","Kia","Hyundai"}

#遍历文章标题-article_catelog
for keyword in final_filtered_keywords:
  for x in catelog_us:
    if x in keyword:
      article_catelog.append("United States")
      break
  for y in catelog_jp:
    if y in keyword:
      article_catelog.append("Japan")
      break
  for z in catelog_eu:
    if z in keyword:
      article_catelog.append("Europe")
      break
  for d in catelog_cn:
    if d in keyword:
      article_catelog.append("China")
      break
  for a in catelog_platform:
    if a in keyword:
      article_catelog.append("Platform")
      break
  for b in catelog_tier1:
    if b in keyword:
      article_catelog.append("Tier1")
      break
  for c in catelog_component:
    if c in keyword:
      article_catelog.append("Component")
      break
  for d in catelog_Korea:
    if d in keyword:
      article_catelog.append("Korea")
      break

tag_count = 0
for title in final_filtered_titles:

  temp_article_tag = []
  for d in tag_key:
    if d in title:
      temp_article_tag.append(d)
  if len(temp_article_tag) == 0:
    new_keyword2 = final_filtered_keywords[tag_count].replace(' EV', '')
    temp_article_tag.append(new_keyword2)
  article_tag.append(temp_article_tag)

  tag_count += 1

#for i in range(len(final_filtered_keywords)):
#  article_tag.append(final_filtered_keywords[i])


#處理一下網址問題
deal_new_list_link = []

for k in range(len(final_filtered_links)):
    if final_filtered_links[k].startswith("https://www.google.com/url?rct=j&sa=t&url="):
        # 使用正則表達式移除指定的部分
        #cleaned_url = re.sub(r'https://www\.google\.com/url\?rct=j&sa=t&url=|&ct=ga&cd=CAIyGmJjMDM0NjQzNmYxOTljYjU6Y29tOmVuOlVT&usg=AOvVaw2_9VR-GCZ_vtziEsB1nH6C', '', final_filtered_links[k])


        parts1 = final_filtered_links[k].split("&url=")

        # 如果切分後的列表有多於一個元素，則將保留最後一個元素
        if len(parts1) > 1:
            cleaned_url = parts1[-1].split("&")[0]
            deal_new_list_link.append(cleaned_url)
        else:
            print("網址格式不正確")

    else:
        # 使用split方法拆分字符串
        parts = final_filtered_links[k].split("&")

        # 保留第一个部分
        result = parts[0]
        deal_new_list_link.append(result)





print(len(final_filtered_titles),len(pass_list_title_zh),len(deal_new_list_link),len(final_filtered_keywords),len(article_catelog),len(article_tag))


from htmldate import find_date
from datetime import datetime, timedelta
import time

final_title_en_filter = []
final_title_zh_filter = []
final_liink_filter = []
final_keyword_filter = []
final_catelog_filter = []
final_tag_filter = []

# 获取当前时间
current_time = datetime.now()

# 遍历每个新闻链接
for x in range(len(final_filtered_titles)):
    try:
        # 使用HTMLDate库来查找新闻发布时间
        date = find_date(deal_new_list_link[x], outputformat='%Y-%m-%d %H:%M:%S')

        # 转换发布时间为datetime对象
        news_time = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')

        # 计算新闻发布时间与当前时间的差值
        time_difference = current_time - news_time

        # 如果新闻发布时间在24小时内，则将该新闻链接添加到recent_news_links列表中
        if time_difference < timedelta(days=1):
            final_title_en_filter.append(final_filtered_titles[x])
            final_title_zh_filter.append(pass_list_title_zh[x])
            final_liink_filter.append(deal_new_list_link[x])
            final_keyword_filter.append(final_filtered_keywords[x])
            final_catelog_filter.append(article_catelog[x])
            final_tag_filter.append(article_tag[x])

        else:
            print("News older than 24 hours:", deal_new_list_link[x])
    except Exception as e:
        print("Error occurred for", deal_new_list_link[x], ":", e)

    # 加入一些延迟，以免对服务器造成过大的负担
    time.sleep(1)  # 在每次请求之间添加 1 秒的延迟

# 打印24小时内的新闻链接
print(len(final_title_en_filter),len(final_title_zh_filter),len(final_liink_filter),len(final_keyword_filter),len(final_catelog_filter),len(final_tag_filter))


import xmlrpc.client
import re
import time

xmlrpc_url = 'http://13.112.222.131//xmlrpc.php'
username = "topeggy"
password = "Qu@nt@eggy202312"

wordpress_post_title = []

while True:  # 無窮迴圈，直到程式成功執行完畢或手動中斷
    try:
        # 建立一個XML-RPC客戶端
        wp = xmlrpc.client.ServerProxy(xmlrpc_url)

        # 使用帐号密码进行登录
        blogs = wp.blogger.getUsersBlogs(username, username, password)
        print("Logged in successfully!")

        # 获取第一个博客的blog_id
        blog_id = blogs[0]['blogid']

        # 获取所有文章
        all_posts = wp.wp.getPosts(blog_id, username, password, {'post_type': 'post', 'number': 9999})
        post_count = len(all_posts)
        print("Total number of posts:", post_count)

        # 获取所有文章标题和内容
        print("\nList of post titles and contents:")
        for post in all_posts:
            # 提取标题部分
            match = re.match(r'(.+?)<br>', post['post_content'])
            if match:
                title = match.group(1)
            else:
                title = post['post_title']

            print("Title:", title)
            wordpress_post_title.append(title)
            #print("Content:", post['post_content'])
            print("-" * 50)

        break  # 若程式成功執行完畢，跳出無窮迴圈

    except (xmlrpc.client.Fault, xmlrpc.client.ProtocolError, ConnectionError) as err:
        print("Error occurred:", err)
        if isinstance(err, xmlrpc.client.ProtocolError) and err.errcode == 500:
            print("Server Error. Retrying in 10 seconds...")
            time.sleep(10)  # 若出現500 Internal Server Error，等待10秒後重新執行
        else:
            print("Retrying in 5 seconds...")
            time.sleep(5)  # 其他錯誤，等待5秒後重新執行


from wordpress_xmlrpc import Client, WordPressPost
from wordpress_xmlrpc.methods.posts import NewPost

# WordPress網站的XML-RPC端點和用戶認證資訊

#xmlrpc_url = 'https://eggy-cabin.com/xmlrpc.php'
xmlrpc_url = 'http://13.112.222.131//xmlrpc.php'

#username = 'Eggy'
#password = '0938009093'

#5d
username = "topeggy"
password = "Qu@nt@eggy202312"

tag_thumbnail_mapping = {
    #us
    'Ford': 1461,'FaradayFuture': 1460,'Vinfast': 1456,'Waymo': 1459,'GM': 1579,'Cruise': 1458,'robotaxi': 3541,'Tesla': 1453, 'Lucid': 7491,
    #jp
    'Toyota': 1455,'Sony-Honda': 1578,'Honda': 1454,'Nissan': 1577,
    #eu
    'Volvo': 1572,'Benz': 1574,'VAG': 1565,'Audi': 1573,'BMW': 1575,'volkswagen': 1576,'Stellantis': 1564,
    #ch
    'BYD': 5349,'Zeeker': 1580,'Li-Auto': 1559,'Geely': 1557,'Nio':4826,'Xpeng':4828,'GreatWall': 1558,'Changan': 1556,'SAICMotor': 1563,
    #platfrom
    'NVidia': 1560,'Qualcomm': 1562,'Intel': 1566,'HorizonRobotics': 1567,'NXP': 1561,
    #tier1
    'Bosch': 1546,'Aisin': 1547,'Hitachi': 1554,'Denso': 1550,'Desay': 1551,'Foxconn': 1553,'Flextronic': 1552,'Valeo': 1555,'Aptiv': 1548,
    #com
    'Inverter': 1570,'ADAS': 1569,'IVI': 1568,
    #korea
    'Kia': 2620,'Hyundai': 2616
}

# 創建新 post 的函數
def create_post(en_title,link,title,post_cate, post_tag, thumbnail_id=None):
    #paramter
    post_content = en_title + " , " + "<br><a href='" + link + "'>Source Link</a>"


    # 連接到WordPress網站
    wp = Client(xmlrpc_url, username, password)

    # 創建新文章
    post = WordPressPost()
    post.title = title
    post.content = post_content
    post.post_status = 'publish'  # 可以設定為'draft'以創建草稿
    post.thumbnail = thumbnail_id

    post.terms_names = {
      'post_tag': [post_tag],
      'category': [post_cate]
    }

    # 將文章發佈到WordPress網站
    wp.call(NewPost(post))

for i in range(len(final_title_en_filter)):
    # 判斷是否有重複
    is_duplicate = False
    for j in range(len(wordpress_post_title)):
        if final_title_en_filter[i] == wordpress_post_title[j]:
            is_duplicate = True
            break

    if is_duplicate:
        print("有重複")
    else:
        thumbnail_id = None
        for tag in final_tag_filter[i]:
            if tag in tag_thumbnail_mapping:
                thumbnail_id = tag_thumbnail_mapping[tag]
                print(tag, thumbnail_id)
                create_post(final_title_en_filter[i], final_liink_filter[i], final_title_zh_filter[i], final_catelog_filter[i], final_tag_filter[i][0], thumbnail_id)
                time.sleep(1)  # 放在這裡可能更合適，確保每次發布間隔一秒
                break  # 找到第一個匹配的標籤後就跳出循環


#for i in range(len(final_title_en_filter)):
  #在這邊判斷是否有無重複
#  for j in range(len(wordpress_post_title)):
#    if final_title_en_filter[i] == wordpress_post_title[j]:
#      print("有重複")
#    else:
#      thumbnail_id = None
#      for tag in final_tag_filter[i]:
#          if tag in tag_thumbnail_mapping:
#              thumbnail_id = tag_thumbnail_mapping[tag]
#              print(tag, thumbnail_id)
#              create_post(final_title_en_filter[i],final_liink_filter[i],final_title_zh_filter[i],final_catelog_filter[i], final_tag_filter[i][0], thumbnail_id)
#              break  # 找到第一個匹配的標籤後就跳出循環
#              time.sleep(1)

import spacy

# 載入spaCy模型
nlp = spacy.load("en_core_web_md")